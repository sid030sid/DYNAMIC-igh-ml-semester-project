currently 
- visualice cluster pcs, tsne with diagnosis overlay https://docs.google.com/document/d/1v6Vw_XCM4By2oI48qXzSwcsU060eufPQpH9f9kKApZA/edit?usp=sharing)
- interpret univariat log regression in takeaways section
    - look at models giving convergence warning: probably cases with just 1 or 0 --> zero deviation or close to zero deviation
    - is missingness explaining the labels?; if yes, it is bad cause these missing values have influence on the clustering. if not, it means that the nan preprocessing has similar effect on clustering as if the missing values were there (check this intuition with annie)
- get diagnoses of interest and the 10 symptoms from sasha which need to be combined because there are for infants and older children

- ask annie: What NAN pre-processing method should ultimately be used?
    - what is the protocol?
        - compare number of consultations per cluster and how consultations move inbetween clusters: are they different? --> if not increase K to find subclusters, if yes:
            - quantative quality check: 
                - compare silhouette score among clustering results
                - is missingness explaining the labels?; if yes, it is bad cause these missing values have influence on the clustering. if not, it means that the nan preprocessing has similar effect on clustering as if the missing values were there (check this intuition with annie)
            - qualitive quality check: look at distribution of diagnoses in clusters. are we clustering more than just the diagnoses? if yes: good, if not: meaningless clustering --> reduce dimension of cluster result to 2d and lay over diagnosis (TSNE vs PCA vs UMAP)


- proper analysis of clusters: demographic, diagnoses distribution




big goals
1. visualize clusters pca tsne (diagnoses nuances: deeper analysis of clusters which are not overlaying with any diagnosis)
    - get diagnoses
2. dashboard syndromic surveillance for swiss tph
    - outlier detection with scientific approach (variance, growth, mean) --> todo for next week: find data sciency approach and discuss with annie
        - for each space time unit: test if percentage of consultations per cluster is different significantly (what if tme space has not enough data for test)
            - how to make time unit: day, week or probabilisitc (= ensuring certain amount of consultations is in time frame)
            - flter for rows whch percentage_consultaton is significantly higher than the percentage_consultaton_thrshold (= mean of percentage_consultation per cluster) --> one sided gaussian test (one sample ttest)
                - H0: percentage_consultaton <= percentage_consultaton_thrshold 
                - H1: percentage_consultaton > percentage_consultaton_thrshold
3. tz
4. optional: extra spatial feature: elevation, rural area or not, (aamir can be of help to get satelitte image. he works with poverty.)

 



- ask about new data 
    - from zsofia she got a answer.csv from 18.03. and i am using one of 13.02. --> only using intervention conultations still valid?
    - from tanzania    
- cluster features with nan values (two ways: kmoidos --> issue: scikit_learn_extra cannot be installed, kprototypes --> issue: requires declaration of categorical features)
    - use different distance method: Gower distance or the Mahalanobis distance
- ask about following todos which depend on direction of research
    - implement kmeans with multiple imputation and k-nearest neighbor imputation since they are mentioned as methods for dealing with structural missingness 

- do base line spatio-temporal clustering:
    - cluster three datasets different regarding how missingness is addressed
        1. symptom and demographic data with NAN values --> use kmeans variation that handles NAN (kmeans++ or fuzzy clustering or GMM)
        3. impute NAN as above. however, we also always add a binary column keeping track if value of respective feature is imputed or not --> analyse if imputation is picked up by clusters
    - compute centroids of shape in data prep notebook
        - find out format of given coordinates in shape file. they seem to be utm
        - make code final data prep notebook --> make a fnial data set stored as pickleee
        - make it possible so that and analysis notbook easily gets spatial dimension
    - make rough analysis of spatial and temporal dimension using hf coordinates and px.density_mapbox with date slider
- research reviews: systematic missingness clustering and clustering of representation (medarchive, padme, google scholar)

ask sascha: what amount of clusters would not make sense, this is the biggest k for the elbow method
how to visualise villages on map best (use shape coordinates or do i need to engineer a coordinate pair) --> try visualising the shapes
    - use interactive heat map for spatial-temporal analysis using "health_facility_longitude", "health_facility_latitude" as location(alternatively: use centre of shape of each village)
        fig = px.density_mapbox(results3, lat='LAT', lon='LON', z='CO2',hover_name = 'LocationName',
                                hover_data = ['altitude', 'SensorUnit_ID','zone'], radius=10,
                                center=dict(lat=47.38, lon=8.5), zoom=9, animation_frame='Date',
                                mapbox_style="stamen-terrain")
        fig.show()
ask sasha:
- which columns are symptoms and which are diagnosis? (get understanding of the used abbreviations)
- which symptoms should i focus on aka which should i use for clustering or should i use all (assume: a is proxy of diagnosis)
- which columns can i get rid of? (to keep: data for hf, patient, symptoms, diagnosis ... else?)
cluster using 
- k-means (as standard)
- Gaussian Mixture models
- co-clustering (suggested by former semester project by Paloma) as it includes spatio-temporal features in the algo
- Agglomerative clustering (according to Paloma too much computational cost)
carbon foot print analysis of project https://pypi.org/project/cumulator/
check for clustering topics in #wishididthat chanel
