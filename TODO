currently
- interpret univariat log regression in takeaways section
- do presentation + continue a bit the code so that one can have visuals to show in the presentation
- proper analysis of clusters: demographic, spatial, temporal
- for each space time unit: test if percentage of consultations per cluster is different significantly (what if tme space has not enough data for test)
    - how to make time unit: day, week or probabilisitc (= ensuring certain amount of consultations is in time frame)
    - flter for rows whch percentage_consultaton is significantly higher than the percentage_consultaton_thrshold --> one sided gaussian test
        - H0: percentage_consultaton <= percentage_consultaton_thrshold 
        - H1: percentage_consultaton > percentage_consultaton_thrshold
 


- domain check up of clusters
    - check top 10 diagnosis and others in clusters (same for syndroms)
        - numericly check occurence of diagnosis in cluster
    - check most prevelent cluster of the day and of the week
    - convenient graph for domain experts


- ask about new data 
    - from zsofia she got a answer.csv from 18.03. and i am using one of 13.02. --> only using intervention conultations still valid?
    - from tanzania    
- cluster features with nan values (two ways: kmoidos --> issue: scikit_learn_extra cannot be installed, kprototypes --> issue: requires declaration of categorical features)
    - use different distance method: Gower distance or the Mahalanobis distance
- ask about following todos which depend on direction of research
    - implement kmeans with multiple imputation and k-nearest neighbor imputation since they are mentioned as methods for dealing with structural missingness 

- do base line spatio-temporal clustering:
    - cluster three datasets different regarding how missingness is addressed
        1. symptom and demographic data with NAN values --> use kmeans variation that handles NAN (kmeans++ or fuzzy clustering or GMM)
        3. impute NAN as above. however, we also always add a binary column keeping track if value of respective feature is imputed or not --> analyse if imputation is picked up by clusters
    - compute centroids of shape in data prep notebook
        - find out format of given coordinates in shape file. they seem to be utm
        - make code final data prep notebook --> make a fnial data set stored as pickleee
        - make it possible so that and analysis notbook easily gets spatial dimension
    - make rough analysis of spatial and temporal dimension using hf coordinates and px.density_mapbox with date slider
- research reviews: systematic missingness clustering and clustering of representation (medarchive, padme, google scholar)

ask sascha: what amount of clusters would not make sense, this is the biggest k for the elbow method
how to visualise villages on map best (use shape coordinates or do i need to engineer a coordinate pair) --> try visualising the shapes
    - use interactive heat map for spatial-temporal analysis using "health_facility_longitude", "health_facility_latitude" as location(alternatively: use centre of shape of each village)
        fig = px.density_mapbox(results3, lat='LAT', lon='LON', z='CO2',hover_name = 'LocationName',
                                hover_data = ['altitude', 'SensorUnit_ID','zone'], radius=10,
                                center=dict(lat=47.38, lon=8.5), zoom=9, animation_frame='Date',
                                mapbox_style="stamen-terrain")
        fig.show()
ask sasha:
- which columns are symptoms and which are diagnosis? (get understanding of the used abbreviations)
- which symptoms should i focus on aka which should i use for clustering or should i use all (assume: a is proxy of diagnosis)
- which columns can i get rid of? (to keep: data for hf, patient, symptoms, diagnosis ... else?)
cluster using 
- k-means (as standard)
- Gaussian Mixture models
- co-clustering (suggested by former semester project by Paloma) as it includes spatio-temporal features in the algo
- Agglomerative clustering (according to Paloma too much computational cost)
carbon foot print analysis of project https://pypi.org/project/cumulator/
check for clustering topics in #wishididthat chanel
